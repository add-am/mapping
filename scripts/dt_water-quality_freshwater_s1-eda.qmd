---
title: "Freshwater Water Quality Exploratory Data Analysis"
subtitle: "A Healthy Waters Partnership Analysis"
description: "Script 1 in a series of script designed to analyse, score, and present freshwater water quality in the Dry Tropics region. The output of this is used in the Dry Tropics Technical Report."
author: "Adam Shand"
format: html
params:
  project_crs: "EPSG:7844"
  target_fyear: 2021 
---

:::{.callout-note}
The Water Quality suite of scripts currently do not adhere to the CamelCase naming rules. This is due to filtering and code that relies on snake_case naming to work. This will take a significant amount of time to overhaul.
:::

# Introduction

The purpose of this script is to prepare and perform EDA for the freshwater water quality data for the dry tropics technical report. Key steps include:

 - Loading all metadata
 - Loading all sampling data
 - Assigning metadata to every sample
 - Performing QA/QC such as checking LOR values, conducting EDA, calculating summary statistics
 - Creating Histograms
 - Creating box plots
 - Creating line plots

# Script Set Up

This script requires multiple core spatial packages, each of which is loaded below.

```{r}
#| label: load packages

#use pacman function to load and install (if required) all other packages
pacman::p_load(tidyverse, glue, here, sf, tmap, janitor, readxl, ggplot2)

```

```{r}

#turn off scientific notation
options(scipen = 999)

#set a path to the data reading location
data_path <- here("data/dt_water-quality_freshwater/raw/")

#establish what financial year we are looking at
current_fyear <- params$target_fyear
proj_crs <- params$project_crs

#get a date variable (this is important for naming as it is anticipated multiple runs of the script will be required).
date <- format(Sys.time(), "%Y-%m-%d")

#create a file path for the year of data that is being looked at
year_folder <- glue("{here()}/outputs/dt_water-quality_freshwater_s1-eda/{current_fyear-1}-{current_fyear}_eda/")

#create that folder
dir.create(year_folder, recursive = T)

#create a file path to help with saving things, make sure to include date
save_path <- glue("{year_folder}/eda_conducted_on_{date}/")

#create folder
dir.create(save_path, recursive = T)

#create additional extension folders from this
dir.create(glue("{save_path}/spreadsheets/"))
dir.create(glue("{save_path}/graphs/"))

```

# Load Data

Data for this script is provided in two spreadsheets: a metadata spreadsheet, and a master spreadsheet. 

The metadata spreadsheet contains (for all current sites):

 - Grouping information for each site (region, environment, basin, sub_basin, watercourse, code),
 - Location information for each site i.e., where the site is (lat, long), and how big is the area it is in,
 - Supplier information (who provides the data),
 - Alternative names the site might be known by,
 - Water quality objective (WQO) information for each site,
 - Limit of reporting (LOR) information for each site,
 - Scaling factor (SF) information for each site,
 - The original source for WQO, LOR, and SF values,
 - plus the above information for all historical sites.
 
The master spreadsheet contains:

 - Codes for all data, to be cross checked against metadata,
 - All sample data for all current sites for as far back as possible (~2019),
 - All currently monitored variables (DIN, TP, Turbidity, DO, and soon to be FRP),
 - Units for each variable, to cross check against metadata,
 - Peripheral variables as need to calculated main variables (e.g., DIN = Ammonia + NOx, or Temp to calculate %DO saturation).

## Metadata Spreadsheet

```{r}
#| label: load metadata

#create list of sheets to target
target_sheets <- c("Current_Sites", "WQO", "LOR", "SF")

#create empty dataframe to hold metadata
site_metadata <- data.frame()

#get the total number of sheets excluding the first one (which is the README sheet)
for (i in 1:length(target_sheets)){
  
  #read in the target sheet
  temp_sheet <- read_excel(glue("{data_path}/dt_wq_freshwater_metadata.xlsx"),
                           sheet = target_sheets[i], na = c("", "NA", "NULL", "null")) 

  #if it is the first sheet(the current sites sheet) then rename it to the main metadata variable
  if (i == 1){site_metadata <- temp_sheet |> select(-c(Site_Name, AKA))} 
  
  else {#otherwise, pivot the data and then merge it onto the main metadata variable
    
    temp_sheet <- pivot_longer(temp_sheet, cols = 2:ncol(temp_sheet), #select data, then target columns
                               names_to = c(".value", "Indicator"), names_pattern = "([A-Za-z]+)_(.+)")
    #.value indicates this part of the col name defines the name of the output column (it overrides the "values_to" arg)
    #names_pattern = how to split. this has two groups marked by brackets: 1. any letter left of underscore, 2. everything else.
    
    #merge onto main
    site_metadata <- merge(site_metadata, temp_sheet)}

}

#add units of measure, direction of failure, and statistic used. These should be able to be crossed check against the human readable pages in the excel metadata spreadsheet if anything seems off
site_metadata <- site_metadata |> 
  mutate(Units = case_when(Indicator == "Turbidity" ~ "NTU", 
                           Indicator %in% c("High_DO", "Low_DO") ~ "%.Sat",
                           T ~ "mg.L"), 
         Failure = case_when(Indicator == "Low_DO" ~ "low", T ~ "high"), Stat = "median")

#clean up
rm(target_sheets, temp_sheet)

```

## Master Data Spreadsheet

The master data sheet is built over a couple of stages:

### Import Datasets

Each of the datasets needs to be imported, they are stored in separate sheets so must be read in separately. 

 !!! need to change this, now do manually - Note the logic to remove the < symbol. These values are handled later using the metadata information.
 - Note the replacement of "aturat" (catches any variation of "saturated" (caps of otherwise)) with the value 100.
 - Note dates: some are provided as date_time, others, only date. The edit drops time and does not affect rows without time attached.

```{r}
#| label: Import datasheets

#get a variable listing all available sheets
sheets <- excel_sheets(glue("{data_path}/dt_wq_freshwater_data_master.xlsx"))

#get the total number of sheets excluding the first one (which is the README sheet)
for (i in 2:length(sheets)){
  
  #read in the target sheet
  temp_sheet <- read_excel(glue("{data_path}/dt_wq_freshwater_data_master.xlsx"), 
                           sheet = sheets[i], na = c("", "NA", "NULL", "null")) |> 
    select(-Site_Name) #drop site name immediately as it is an uncontrolled variable only included for the human reader
  
  #assign the correct name, remove < symbols, and convert columns to numeric
  assign(sheets[i], temp_sheet |>
           mutate(#across(c(3:ncol(temp_sheet)), ~ str_replace(.x, "<", "")), #this should now be obsolete
                  across(contains("DO"), ~ str_replace(.x, ".*aturat.*", "100")),
                  across(c(3:ncol(temp_sheet)), as.numeric),
                  Date = Date + seconds(1), #add one second to avoid date-time error
                  Date = ymd_hms(Date), FY = get_fy(Date), .after = Date) |> 
           filter(FY > 2012,
                  FY <= current_fyear)) #to allow for specific reporting year targeting
}

#clean up
rm(sheets, temp_sheet, i)

```

### Special Treatment of Datasets

Some of the datasets have unique treatments that they received before they can be combined together, for example, below we calculate the saturation of Dissolved Oxygen from the mg/L of Dissolved Oxygen. In this current script, this is the only special treatment that is required.

```{r}
#| label: calculate dissolved oxygen

#The TCC Other datasheet "special" process is converting DO mg/L to DO % saturation, read in a conversion table
conversion_table_DO <- read_csv(glue("{data_path}/conversion_table_dissolved_oxygen.csv"))

#fit a linear model to the data
DO_lm <- lm(formula = DO_solubility_at_saturation ~ poly(Temperature, 3), data = conversion_table_DO)

#divided the recorded DO_mg/L by the predicted max DO saturation at the recorded temperature and times by 100. As an example, according to our linear model, at 20C, for DO to be 100% saturated, the concentration must be 9.218633mg/L. Thus if our recorded DO is 9.218633mg/L we can convert that to 100% saturated. If our DO is less that 9.218633, it calculates as less than 100% saturated, and similarly for if recorded DO is greater than 9.218633.
TCC_Other <- TCC_Other |> 
  mutate(`DO_%.Sat` = (DO_mg.L/predict(DO_lm, data.frame(Temperature = Temperature_C), type = "response")) * 100) |> 
  select(-c(Temperature_C,DO_mg.L))

#clean up
rm(conversion_table_DO, DO_lm)

```

### Bind Datasheets

now all sheets can be combined and provided with metadata and the finishing touches added. This includes adjusting values that are at or below the limit of reporting.

```{r}
#| label: bind datasheets

#Each of the spreadsheets (now aligned) can be given the same process
freshwater_wq <- bind_rows(DES, TCC_Wastewater, TCC_Other, CLMP) 

#create extra indicator columns
freshwater_wq_all <- freshwater_wq |> rowwise() |> 
  mutate(DIN_mg.L = sum(Ammonia_mg.L, NOX_mg.L, na.rm = T),
         DIN_mg.L = case_when(DIN_mg.L == 0 ~ NA, T ~ DIN_mg.L),
         "Low_DO_%.Sat" = `DO_%.Sat`) |> #calculate DIN, and create a low DO column
  rename("High_DO_%.Sat" = `DO_%.Sat`) |> #create a high DO column
  ungroup()

#pivot data around indicators and units, remove values that are NA
freshwater_wq_all <- freshwater_wq_all |> 
  pivot_longer(cols = 4:ncol(freshwater_wq_all), names_to = "Indicator", values_to = "Values") |> #pivot data longer
  separate_wider_regex("Indicator", c("Indicator" = ".*", "_", "Units" = ".*")) |> #split names into indicator and units
  group_by(Code, Date, FY, Indicator, Units) |> 
  summarise(Values = mean(Values)) |> ungroup() |>  #get daily mean
  left_join(site_metadata) |> 
  filter(!is.na(Values))

#clean up
rm(DES, TCC_Wastewater, TCC_Other, CLMP, freshwater_wq)

```

# QA/QC Checks

We can now take this opportunity to perform the required QA/QC Checks, at the moment this includes:

 - Units
 - Locations and Codes
 - LOR/value comparison
 - LOR/WQO comparison
 
Please add more as you see fit.

## Unit Check

First we will check the units, we are looking to detect if any of the indicators have more than one set of units assigned.

```{r}
#| label: Unit Check
#| warning: true

#group by indicator and units
unit_check <- freshwater_wq_all |> 
  group_by(Indicator, Units) |> 
  summarize(.groups = "drop")

#then check if the number of indicators is equal to the number of indicator-unit pairs
if (length(unique(unit_check$Indicator)) == n_distinct(unit_check)){
  
  print("Test passed. No erroneous indicator units.")
  
  rm(unit_check)
  
} else {
  
  #save csv
  write_csv(unit_check, glue("{save_path}/spreadsheets/unit_check.csv"))

stop("Test fail, an indicator has data with two different units. Raw data might be provided with two 
     different units, or the units listed in the metadata may not match the master data. Review the
     'unit_check' table for more information.")
}

```

## Locations and Codes

We will check the number of locations found in the dataset matches what we expect to find (which for freshwater is 16 unique watercourses and 24 unique codes (sites)).

```{r}
#| label: Location and Code Check
#| warning: true

#group by the different levels of location
location_check <- freshwater_wq_all |> 
  group_by(Region, Environment, Basin, Sub_Basin, Watercourse) |> 
  summarize(.groups = "drop")

#then check if we have the right number of locations
if (nrow(location_check) == 16 & !any(is.na(location_check))){
  
  print("Test passed. Correct number of locations.")
  
  rm(location_check)
  
} else {
  
  #save csv
  write_csv(location_check, glue("{save_path}/spreadsheets/location_check.csv"))

stop("Test fail, currently the DT report should have 16 unique watercourses, please cross check if 
     there are locations missing, or if there have been new locations added. Review the 
     'location_check' table for more information.")
}

#group by watercourse and code
code_check <- freshwater_wq_all |> 
  group_by(Watercourse, Code) |> 
  summarize(.groups = "drop")

#then check if we have the right number of watercourses and codes
if (nrow(code_check) == 24 & !any(is.na(code_check))){
  
  print("Test passed. Correct number of sites.")
  
  rm(code_check)
  
} else {
  
  #save csv
  write_csv(code_check, glue("{save_path}/spreadsheets/code_check.csv"))

stop("Test fail, currently the DT report should have 24 unique codes(sites), please cross check if 
     there are codes missing, or if there have been new codes added. Review the 'code_check' table
     for more information.")
}

```

## LOR/Value Comparison

We will now compare the LOR values against the concentration values recorded in the master spreadsheet. What we are trying to spot is how many of the concentrations values are half (or less) of the LOR. If lots of the concentration values are half the LOR then this is a warning sign that the LOR is likely not low enough and not fit for purpose.

```{r}
#| label: LOR Value check
#| warning: true

#check how many values are half of the LOR compared to the total number of values for the reporting period
value_lor_check <- freshwater_wq_all |> 
  filter(FY == current_fyear) |> 
  mutate(`Value_Halved?` = case_when(Values <= LOR ~ "Value_Halved", 
                                     T ~ "Value_Unchanged")) |> 
  group_by(Code, Indicator, `Value_Halved?`) |> 
  summarise(Count = n(), .groups = "drop") |> 
  pivot_wider(names_from = `Value_Halved?`, values_from = Count) |> 
  filter(!is.na(Value_Halved))

if (!any(value_lor_check$Value_Halved > value_lor_check$Value_Unchanged)){
  
  print("Minimal values were changed.")
  
  rm(value_lor_check)
  
} else {
  
  #save the value lor check table
  write_csv(value_lor_check, glue("{save_path}/spreadsheets/value_lor_check.csv"))

warning("At >= one site, there are more values that have been changed to half the LOR than have 
been left unchanged. This is not inherently wrong, but should be double checked. Check the 
'value_lor_check' table for a list of all sites with changed values.")

}

```

## LOR/WQO Comparison

Finally we will compare the LOR and WQO values. This an important comparison as if the WQO value is equal to or less than the LOR value it becomes impossible to determine if the actual concentration values are equal to or less than the WQO value. For example:

 - If LOR = 1,
 - And WQO = 0.5,
 - The lowest a concentration can be recorded is <1 (half LOR).
 - Therefore, is the concentration (<1) lower than the WQO (0.5)?
 - It is impossible to know.
 
The below code chunk will flag any site that has:
 - an LOR <= WQO and,
 - more than half of the concentration values also <= WQO (i.e. cannot be interpreted). 
 
For any site that is flagged, the offending indicator will be removed.

```{r}
#| label: LOR WQO check
#| warning: true

#check if the LOR is equal to or greater than the WQO, whilst also checking if any values are also in this range
wqo_lor_check_p1 <- freshwater_wq_all |> 
  filter(FY == current_fyear, Indicator != LOR) |> #this gets current year and ignore FRP
  mutate(Comparison = case_when(LOR >= WQO & Values > LOR ~ "Val_Warn",
                                LOR >= WQO & Values <= LOR ~ "Val_Fail",
                                T ~ "Val_Pass")) |>
  group_by(Code, Indicator, LOR, WQO, Comparison) |> 
  summarise(Count = n(), .groups = "drop") |>
  pivot_wider(names_from = Comparison, values_from = Count)

#clean up the main dataset by removing ammonia and NOX as they are no longer needed after this check has been completed.
freshwater_wq_all <- freshwater_wq_all |> filter(!Indicator %in% c("Ammonia", "NOX"))

#check if warning and fail values exists
if (any("Val_Fail" %in% colnames(wqo_lor_check_p1))){

  #conduct checks for each indicator to determine if enough samples are above LOR/WQO to be allowed to use the site
  wqo_lor_check_p2 <- wqo_lor_check_p1 |> filter(Indicator != "DIN") |> 
    mutate(across(everything(), ~ replace_na(.x, 0)),
           Indicator = case_when(Indicator %in% c("Ammonia", "NOX") ~ "DIN", 
                                 T ~ Indicator)) |> 
    group_by(Code, Indicator) |> 
    summarise(across(contains("Val"), ~ sum(.x)),
              Keep_Site = case_when(Val_Fail < (if(exists("wqo_lor_check_p1$Val_Warn")){Val_Warn + Val_Pass} else {Val_Pass}) ~ "Keep", 
                                    T ~ "Remove"), .groups = "drop") |> 
    ungroup()
  
  #save the lor WQO check table
  write_csv(wqo_lor_check_p2, glue("{save_path}/spreadsheets/wqo_lor_check.csv"))
  
  #check if any sites need to be removed
  if (!any(str_detect(wqo_lor_check_p2$Keep_Site, "Rem"))){
    
    #if no sites need to be removed save the data
    write_csv(freshwater_wq_all, glue("{save_path}/spreadsheets/freshwater_wq_all.csv"))
  
    print("Test passed. No sites need to be removed.")
    
  } else { #if warning and fail values outnumber values that pass then remove those rows
  
    #to remove problem sites create a table containing the code-indicator pair of any failed sites
    removal_table <- wqo_lor_check_p2 |> 
      filter(Keep_Site == "Remove") |> 
      select(Code, Indicator)
  
    #use the full dataset to add all information about the code-indicator pair of any failed sites
    removal_table <- semi_join(filter(freshwater_wq_all, FY == current_fyear), removal_table, by = join_by(Code, Indicator))
  
    #subtract from the main table, any rows that appear in the removal table
    freshwater_wq_removed <- suppressMessages(anti_join(freshwater_wq_all, removal_table))
      
    #save the before and after site removal tables
    write_csv(freshwater_wq_all, glue("{save_path}/spreadsheets/freshwater_wq_all_pre_site_removal.csv"))
    write_csv(freshwater_wq_removed, glue("{save_path}/spreadsheets/freshwater_wq_all_post_site_removal.csv"))
    
    #save the table that lists what was actually removed
    write_csv(removal_table, glue("{save_path}/spreadsheets/freshwater_wq_all_sites_removed.csv"))
    
    warning("At >= one site, the LOR is >= WQO AND >= 50% of the values are <= LOR. This means at 
    least one site has failed. The offending sites and indicators have been removed, however the 
    code will provide a warning here so the user is aware and can inspect the cause of the issue.")

  }
  
} else {#if there are no fail values
  
  #save the dataset
  write_csv(freshwater_wq_all, glue("{save_path}/spreadsheets/freshwater_wq_all.csv"))
  
  print("Test passed. No sites need to be removed.")

}    

```

The code chunk above has automatically removed the offending indicators from the offending sites. Now, in the code chunk below, we will overwrite the main freshwater dataset with the dataset we just created that has some values removed.

```{r}
#| label: overwrite old data

#if data needed to be removed, store the main dataset as "old" and assign the updated dataset back to the main dataset
if (exists("freshwater_wq_removed")){
  
  freshwater_wq_pre_removal <- freshwater_wq_all
  
  freshwater_wq_all <- freshwater_wq_removed}

```

# EDA Checks

After some mostly automated QA/QC we will now conduct some exploratory data analysis to see if we can spot anything wrong with this years data.

```{r}
#| label: select this years data

freshwater_wq_cy <- freshwater_wq_all |> filter(FY == current_fyear)

```

## Histograms

Next we will create histograms for this years data, e.g.:

```{r}
#| label: EDA histograms

for (i in unique(freshwater_wq_cy$Indicator)){#for each indicator
  
  #get only that indicator
  target_data <- freshwater_wq_cy |> filter(Indicator == i)
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Values, color = Code, fill = Code)) +
    geom_histogram(alpha = 0.6, bins = 100) +
    viridis::scale_fill_viridis(discrete = T) +
    viridis::scale_colour_viridis(discrete = T) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
      xlab(glue("{i}")) +
      ylab("Count") +
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{save_path}/graphs/{i}_histogram.png"))
  
}

#show last plot
temp_plot

```

## Boxplots

and as a box plots, e.g.:

```{r}
#| label: EDA Boxplots

#create custom log function for tick breaks
base_breaks <- function(n = 10){
    function(x) {
        axisTicks(log10(range(x, na.rm = TRUE)), log = TRUE, n = n)
    }
}

for (i in unique(freshwater_wq_cy$Indicator)){#for each indicator
  
  #get only that indicator
  target_data <- freshwater_wq_cy |> filter(Indicator == i)
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Indicator, y = Values, fill = Code)) +
    geom_boxplot(alpha = 0.6) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
    xlab("") +
    scale_y_continuous(trans = scales::log_trans(), breaks = base_breaks(),
                   labels = prettyNum) +
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{save_path}/graphs/{i}_boxplot.png"))
  
}

#show last plot
temp_plot

```

## Lineplots

and some line plots, e.g.:

```{r}
#| label: EDA line plots

for (i in unique(freshwater_wq_cy$Indicator)){#for each indicator
  
  #get only that indicator and convert from ymd_hms to ymd only
  target_data <- freshwater_wq_cy |> filter(Indicator == i) |> mutate(Date = ymd(substr(Date, 1, 10)))
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Date, y = Values, color = Code)) +
    geom_line() +
    geom_point(size = 0.4) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
    xlab(glue("{i}")) +
    scale_x_date(breaks = scales::pretty_breaks(n = 2))+
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{save_path}/graphs/{i}_lineplot.png"))
  
}

#show last plot
temp_plot

```

## Rainfall

We can then look at the timing of rainfall and when the samples were taken. (Note that to obtain rainfall data we have to take a slight detour).

```{r}
#| label: rainfall eda

#first we will need to load in a few extra packages to do some spatial work
library(sf)
library(terra)
library(exactextractr)

#turn off spherical geometry
sf_use_s2(F)

#read in the northern three data and cut down to freshwater dry tropics only
dry_tropics <- st_read(here("data/n3_region-builder/n3_region.gpkg")) |> 
  filter(environment == "Freshwater", region == "Dry Tropics") |> rename(basin = basin_or_zone)

#set a path to the saved output
path <- glue("{data_path}/../rainfall/sub_basin_rainfall.csv")

if (file.exists(path) == T){ #if it exists, read it in
  
  sub_basin_rain <- read_csv(path)
  
} else { #otherwise make it 
  
  #get list of rainfall files
  file_list <- list.files(glue("{data_path}/../rainfall"), pattern = "rain_day_20", full.names = TRUE, recursive = TRUE)
  
  #combine all files
  all_rain <- rast(file_list)
  
  #update crs info
  crs(all_rain) <- proj_crs

  #create a buffer around the dry tropics
  buff <- st_buffer(dry_tropics, units::set_units(0.05, degree))

  #use trim to remove NAs, and mask to designate area
  dt_rain <- trim(mask(all_rain, vect(buff)))
  
  #save the cut down version
  writeCDF(dt_rain, glue("{data_path}/../rainfall/rain_day_all.nc"), overwrite = T)
  
  #read in the cut down version
  dt_rain <- rast(glue("{data_path}/../rainfall/rain_day_all.nc"))
  
  #create a tibble of layer names and dates
  name_date <- tibble("layer_name" = names(dt_rain), "layer_date" = time(dt_rain))
  
  #extract values for each subbasin for each day
  sub_basin_rain <- tibble(exact_extract(dt_rain, dry_tropics, fun = "mean", append_cols = T))
  
  #fix column names
  names(sub_basin_rain) <- sub('mean.', '', names(sub_basin_rain))

  #pivot the data and round values, then use left_join() to add layer dates, and clean up other columns
  sub_basin_rain <- pivot_longer(sub_basin_rain, cols = 6:ncol(sub_basin_rain),
                                 names_to = "layer_name", values_to = "mean_rainfall") |> 
    left_join(name_date, by = "layer_name") |> 
    rename(Sub_Basin = sub_basin_or_sub_zone, Rainfall = mean_rainfall) |> 
    mutate(Date = ymd_hms(paste(layer_date, "23:00:00")),
           Sub_Basin = case_when(str_detect(Sub_Basin, "Lower") ~ "Lower Ross River",
                                 str_detect(Sub_Basin, "Upper") ~ "Upper Ross River",
                                 str_detect(Sub_Basin, "Palum") ~ "Paluma Dam",
                                 T ~ Sub_Basin)) |> 
    select(Sub_Basin, Rainfall, Date)
  
  #save
  write_csv(sub_basin_rain, path)
  
}

#clean up
rm(path)

```

And then we can then create the plots

```{r}
#| label: create rainfall plots

for (i in unique(freshwater_wq_all$Indicator)){#for each indicator
  for (j in unique(freshwater_wq_all$Sub_Basin)){#and each sub basin
    
    #filter for the data we want
    freshwater_targeted <- freshwater_wq_all |> filter(Indicator == i, Sub_Basin == j)
    rainfall_targeted <- sub_basin_rain |> filter(Sub_Basin == j, Date >= min(freshwater_targeted$Date))
    
    if (all(is.na(freshwater_targeted$Values))){#if all values are NA, skip the loop
      
    } else { #create the plot
    
      #work out the coefficent between each dataset
      coeff <- max(rainfall_targeted$Rainfall, na.rm = T)/max(freshwater_targeted$Values, na.rm = T)
      
      #Define the palette without "Blue"
      original_palette <- rev(RColorBrewer::brewer.pal(9, "YlOrRd"))
      
      #Add "Blue" to the beginning of the palette
      custom_palette <- c("blue", original_palette)
      
      #Identify unique values in the 'category' variable
      unique_codes <- c("Rainfall", unique(freshwater_targeted$Code))
      
      #Create a named vector associating colors with unique values
      color_mapping <- setNames(custom_palette[1:length(unique_codes)], unique_codes)
      
      #plot data
      temp_plot <- ggplot() +
        geom_line(data = rainfall_targeted, aes(x = Date, y = Rainfall/coeff, color = "Rainfall")) +
        geom_line(data = freshwater_targeted, aes(x = Date, y = Values, color = Code)) +
        theme_bw() +
        theme(panel.spacing = unit(0.1, "lines"),
              strip.text.x = element_text(size = 8)) +
        scale_x_datetime(breaks = scales::pretty_breaks(n = length(unique(year(freshwater_targeted$Date))))) +
        scale_y_continuous(name = glue("{i} ({freshwater_targeted$Units})"), 
                           sec.axis = sec_axis(~.*coeff, name = "Rainfall (mm)")) +
        labs(color = "Legend") +
        scale_colour_manual(values = color_mapping)
      
      #save plot
      ggsave(glue("{save_path}/graphs/{i}_vs_rainfall_{j}.png"), temp_plot)
      
    }
  }
}

temp_plot

```

# Action Outstanding EDA

Earlier in the script we conducted preliminary QA/QC and were also able to automated some of the common actions required. However after conducting preliminary EDA there is no easy way to automate the actions required by the findings of this. Thus this section is the custom response to the EDA findings that were determined for the **2023** (financial year) data.

**It is important to note that this section should manually be rewritten each year to specifically address the EDA finding of the year.**

To make sure we don't miss anything we will go indicator by indicator:

 - DIN: Nothing major to note.
    + In the Box plots almost all sites have outliers. However these outliers are caused by almost all data being at the LOR and only a few being above the LOR, rather than crazy high/low values.
    + The Bohle sites are the only ones that show notable DIN values, this makes sense as they are downstream of a significant urban nitrogen source.
    + Rainfall and peak values match up roughly equally.
    + A max DIN value of 9.6 was recorded at BOH22.3 which is also realistic. 
 - DO: Nothing major to note.
    + Although the highest value as 144 - this is not an unrealistic measure. Similarly, in the boxplots it looks like DO zeros out, but the min value is actually 23.3, note the Y axis.
    + Rainfall and peak values match up roughly equally.
 - FRP: Not currently used in the report.
    + No need to critically analyse.
 - TP: Nothing major to note.
    + Similar to DIN, only Bohle sites show elevated values. This tracks due to their location. They also appear to report similar highs and lows at similar times.
    + Rainfall and peak values match up roughly equally.
    + A maximum value of 6.3 is realistic.
 - Turbidity: Nothing major to note
    + A maximum value of 166 was recorded at BOH18.1 This is within an expect measure of NTU.
    + AltC7.0 has recorded significantly higher values than neighboring creeks however is likely influenced by surrounding landuse.
    + Rainfall and peak values match up roughly equally.

```{r}
#| label: action EDA findings

print("This year, no EDA findings required action.")

```

# Save Data

Now that all QA/QC and EDA actions are completed we can save the processed data in the main data folder, ready for the main analysis script. This save step is quite important as we dont want to overwrite old dataset prematurely, thus we will include the year of data that was targeted for the eda checks.

Also, once again it should be noted that the dataset may have had some values removed during the LOR/WQO QA/QC step. If this is the case the dataset will contain an extra bit of information in the file name stating as such.

```{r}
#| label: save data to main data folder

if (exists("freshwater_wq_removed")) {# a removed and non-removed set exists, save both
  
  #save the data with the removed sites
  write_csv(freshwater_wq_all, 
            glue("{here()}/data/dt_water-quality_freshwater/processed/{current_fyear-1}-{current_fyear}_freshwater_wq_all_sites_removed.csv"))
  
  #save the original without the removed sites - this is useful for inspecting at a later date
  write_csv(freshwater_wq_pre_removal, 
            glue("{here()}/data/dt_water-quality_freshwater/processed/{current_fyear-1}-{current_fyear}_freshwater_wq_all_pre_removal.csv"))

} else {#otherwise just save the one
  
  write_csv(freshwater_wq_all, 
            glue("{here()}/data/dt_water-quality_freshwater/processed/{current_fyear-1}-{current_fyear}_freshwater_wq_all.csv"))

}

```

