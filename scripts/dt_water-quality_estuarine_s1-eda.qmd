---
title: "Estuarine Water Quality Exploratory Data Analysis"
subtitle: "A Healthy Waters Partnership Analysis"
description: "Script 1 in a series of script designed to analyse, score, and present estuarine water quality in the Dry Tropics region. The output of this is used in the Dry Tropics Technical Report."
author: "Adam Shand"
format: html
params:
  project_crs: "EPSG:7844"
  target_fyear: 2023 
---

:::{.callout-note}
The Water Quality suite of scripts currently do not adhere to the CamelCase naming rules. This is due to filtering and code that relies on snake_case naming to work. This will take a significant amount of time to overhaul.
:::

# Introduction

The purpose of this script is to prepare and perform EDA for the estuarine water quality data for the dry tropics technical report. Key steps include:

 - Loading all metadata
 - Loading all sampling data
 - Assigning metadata to every sample
 - Performing QA/QC such as checking LOR values, conducting EDA, calculating summary statistics
 - Creating Histograms
 - Creating box plots
 - Creating line plots

# Script Set Up

This script requires multiple core spatial packages, each of which is loaded below.

```{r}
#| label: load packages

#use pacman function to load and install (if required) all other packages
pacman::p_load(tidyverse, glue, here, sf, tmap, janitor, readxl, ggplot2)

```


```{r}
#| label: global vars and initial setup
#
#turn off scientific notation
options(scipen = 999)

#set a path to the data reading location
data_path <- here("data/dt_water-quality_estuarine/raw/")

#establish what financial year we are looking at
current_fyear <- params$target_fyear

#get a date variable (this is important for naming as it is anticipated multiple runs of the script will be required).
date <- format(Sys.time(), "%Y-%m-%d")

#create a file path for the year of data that is being looked at
year_folder <- glue("{here()}/outputs/dt_water-quality_estuarine_s1-eda/{current_fyear-1}-{current_fyear}_eda/")

#create that folder
dir.create(year_folder, recursive = T)

#create a file path to help with saving things, make sure to include date
save_path <- glue("{year_folder}/eda_conducted_on_{date}/")

#create folder
dir.create(save_path, recursive = T)

#create additional extension folders from this
dir.create(glue("{save_path}/spreadsheets/"))
dir.create(glue("{save_path}/graphs/"))

```

# Load Data

Data for this script is provided in two spreadsheets: a metadata spreadsheet, and a master spreadsheet. 

The metadata spreadsheet contains (for all current sites):

 - Grouping information for each site (region, environment, basin, sub_basin, watercourse, code),
 - Location information for each site i.e., where the site is (lat, long), and how big is the area it is in,
 - Supplier information (who provides the data),
 - Alternative names the site might be known by,
 - Water quality objective (WQO) information for each site,
 - Limit of reporting (LOR) information for each site,
 - Scaling factor (SF) information for each site,
 - The original source for WQO, LOR, and SF values,
 - plus the above information for all historical sites.
 
The master spreadsheet contains:

 - Codes for all data, to be cross checked against metadata,
 - All sample data for all current sites for as far back as possible (~2019),
 - All currently monitored variables (DIN, TP, Turbidity, DO, and soon to be FRP),
 - Units for each variable, to cross check against metadata,
 - Peripheral variables as need to calculated main variables (e.g., DIN = Ammonia + NOx, or Temp to calculate %DO saturation).

## Metadata Spreadsheet

```{r}
#| label: load metadata

#create list of sheets to target
target_sheets <- c("Current_Sites", "WQO", "LOR", "SF")

#create empty dataframe to hold metadata
site_metadata <- data.frame()

#get the total number of sheets excluding the first one (which is the README sheet)
for (i in 1:length(target_sheets)){
  
  #read in the target sheet
  temp_sheet <- read_excel(glue("{data_path}/dt_wq_estuarine_metadata.xlsx"),
                           sheet = target_sheets[i], na = c("", "NA", "NULL", "null")) 

  #if it is the first sheet(the "Current_Sites" sheet) then rename it to the main metadata variable
  if (i == 1){site_metadata <- temp_sheet |> select(-c(Site_Name, AKA))} 
  
  else {#otherwise, pivot the data and then merge it onto the main metadata variable
    
    temp_sheet <- pivot_longer(temp_sheet, cols = 2:ncol(temp_sheet), #select data, then target columns
                               names_to = c(".value", "Indicator"), names_pattern = "([A-Za-z]+)_(.+)")
    #.value indicates this part of the col name defines the name of the output column (it overrides the "values_to" arg)
    #names_pattern = how to split. this has two groups marked by brackets: 1. any letter left of underscore, 2. everything else.
    
    #merge onto main
    site_metadata <- merge(site_metadata, temp_sheet)}

}

#add units of measure, direction of failure, and statistic used. These should be able to be crossed check against the human readable pages in the excel metadata spreadsheet if anything seems off
site_metadata <- site_metadata |> 
  mutate(Units = case_when(Indicator == "Turbidity" ~ "NTU", 
                           Indicator %in% c("High_DO", "Low_DO") ~ "%.Sat",
                           T ~ "mg.L"), 
         Failure = case_when(Indicator == "Low_DO" ~ "low", T ~ "high"), Stat = "median")

#clean up
rm(target_sheets, temp_sheet)

```

## Master Data Spreadsheet

The master data sheet is built over a couple of stages:

### Import Datasets

Each of the datasets needs to be imported, they are stored in separate sheets so must be read in separately. 

!!! need to change this, now do manually - Note the logic to remove the < symbol. These values are handled later using the metadata information.
 - Note the replacement of "aturat" (catches any variation of "saturated" (caps of otherwise)) with the value 100.
 - Note dates: some are provided as date_time, others, only date. The edit drops time and does not affect rows without time attached.

```{r}
#| label: Import datasheets

#get a variable listing all available sheets
sheets <- excel_sheets(glue("{data_path}/dt_wq_estuarine_data_master.xlsx"))

#get the total number of sheets excluding the first one (which is the README sheet)
for (i in 2:length(sheets)){
  
  #read in the target sheet
  temp_sheet <- read_excel(glue("{data_path}/dt_wq_estuarine_data_master.xlsx"), 
                           sheet = sheets[i], na = c("", "NA", "NULL")) |> 
    select(-Site_Name) #drop site name immediately as it is an uncontrolled variable only included for the human reader
  
  #assign the correct name, remove < symbols, and convert columns to numeric
  assign(sheets[i], temp_sheet |>
           mutate(across(c(3:ncol(temp_sheet)), ~ str_replace(.x, "<", "")),
                  across(contains("DO"), ~ str_replace(.x, ".*aturat.*", "100")),
                  across(c(3:ncol(temp_sheet)), as.numeric),
                  Date = Date + seconds(1), #add one second to every time to avoid the midnight error
                  Date = ymd_hms(Date), FY = get_fy(Date), .after = Date) |> 
           filter(FY > 2012,
                  FY <= current_fyear))
}

#clean up
rm(sheets, temp_sheet, i)

```

### Bind Datasheets

now all sheets can be combined and provided with metadata and the finishing touches added. This includes adjusting values that are at or below the limit of reporting.

```{r}
#| label: bind datasheets

#Each of the spreadsheets (now aligned) can be given the same process
estuarine_wq <- bind_rows(DES, Ornatas, POTL, TCC) 

#create extra indicator columns
estuarine_wq_all <- estuarine_wq |> 
  mutate(DIN_mg.L = Ammonia_mg.L + NOX_mg.L, "Low_DO_%.Sat" = `DO_%.Sat`) |> #calculate DIN, and create a low DO column
  rename("High_DO_%.Sat" = `DO_%.Sat`) #create a high DO column

#pivot data around indicators and units
estuarine_wq_all <- estuarine_wq_all |> 
  pivot_longer(cols = 4:ncol(estuarine_wq_all), names_to = "Indicator", values_to = "Values") |> #pivot data longer
  separate_wider_regex("Indicator", c("Indicator" = ".*", "_", "Units" = ".*")) |> #split names into indicator and units
  group_by(Code, Date, FY, Indicator, Units) |> 
  summarise(Values = mean(Values)) |> ungroup() |>  #get daily mean
  left_join(site_metadata) |> 
  filter(!is.na(Values))#|> #add metadata
  #mutate(Values = case_when(Values <= LOR ~ (0.5*LOR), T ~ Values))

#clean up
rm(DES, Ornatas, POTL, TCC, estuarine_wq)

```

# QA/QC Checks

We can now take this opportunity to perform the required QA/QC Checks, at the moment this includes:

 - Units
 - Locations and Codes
 - LOR/value comparison
 - LOR/WQO comparison
 
Please add more as you see fit.

## Unit Check

First we will check the units, we are looking to detect if any of the indicators have more than one set of units assigned.

```{r}
#| label: Unit Check
#| warning: true

#group by indicator and units
unit_check <- estuarine_wq_all |> 
  group_by(Indicator, Units) |> 
  summarize(.groups = "drop")

#then check if the number of indicators is equal to the number of indicator-unit pairs
if (length(unique(unit_check$Indicator)) == n_distinct(unit_check)){
  
  print("Test passed. No erroneous indicator units.")
  
  rm(unit_check)
  
} else {
  
  #save csv
  write_csv(unit_check, glue("{save_path}/spreadsheets/unit_check.csv"))

stop("Test fail, an indicator has data with two different units. Raw data might be provided with two 
     different units, or the units listed in the metadata may not match the master data. Review the
     'unit_check' table for more information.")
}

```

## Locations and Codes

We will check the number of locations found in the dataset matches what we expect to find (which for estuarine is 13 unique watercourses and 23 unique codes (sites)).

```{r}
#| label: Location and Code Check
#| warning: true

#group by the different levels of location
location_check <- estuarine_wq_all |> 
  group_by(Region, Environment, Basin, Sub_Basin, Watercourse) |> 
  summarize(.groups = "drop")

#then check if we have the right number of locations
if (nrow(location_check) == 13 & !any(is.na(location_check))){
  
  print("Test passed. Correct number of locations.")
  
  rm(location_check)
  
} else {
  
  #save csv
  write_csv(location_check, glue("{save_path}/spreadsheets/location_check.csv"))

stop("Test fail, currently the DT report should have 13 unique watercourses, please cross check if 
     there are locations missing, or if there have been new locations added. Review the 
     'location_check' table for more information.")
}

#group by watercourse and code
code_check <- estuarine_wq_all |> 
  group_by(Watercourse, Code) |> 
  summarize(.groups = "drop")

#then check if we have the right number of watercourses and codes
if (nrow(code_check) == 23 & !any(is.na(code_check))){
  
  print("Test passed. Correct number of sites.")
  
  rm(code_check)
  
} else {
  
  #save csv
  write_csv(code_check, glue("{save_path}/spreadsheets/code_check.csv"))

stop("Test fail, currently the DT report should have 23 unique codes(sites), please cross check if 
     there are codes missing, or if there have been new codes added. Review the 'code_check' table
     for more information.")
}

```

## LOR/Value Comparison

We will now compare the LOR values against the concentration values recorded in the master spreadsheet. What we are trying to spot is how many of the concentrations values are half (or less) of the LOR. If lots of the concentration values are half the LOR then this is a warning sign that the LOR is likely not low enough and not fit for purpose.

```{r}
#| label: LOR Value check
#| warning: true

#check how many values are half of the LOR compared to the total number of values for the reporting period
value_lor_check <- estuarine_wq_all |> 
  filter(FY == current_fyear) |> 
  mutate(`Value_Halved?` = case_when(Values <= LOR ~ "Value_Halved", 
                                     T ~ "Value_Unchanged")) |> 
  group_by(Code, Indicator, `Value_Halved?`) |> 
  summarise(Count = n(), .groups = "drop") |> 
  pivot_wider(names_from = `Value_Halved?`, values_from = Count) |> 
  filter(!is.na(Value_Halved))

if (!any(value_lor_check$Value_Halved > value_lor_check$Value_Unchanged)){
  
  print("Minimal values were changed.")
  
  rm(value_lor_check)
  
} else {
  
  #save the value lor check table
  write_csv(value_lor_check, glue("{save_path}/spreadsheets/value_lor_check.csv"))

warning("At >= one site, there are more values that have been changed to half the LOR than have 
been left unchanged. This is not inherently wrong, but should be double checked. Check the 
'value_lor_check' table for a list of all sites with changed values.")

}

```

## LOR/WQO Comparison

Finally we will compare the LOR and WQO values. This an important comparison as if the WQO value is equal to or less than the LOR value it becomes impossible to determine if the actual concentration values are equal to or less than the WQO value. For example:

 - If LOR = 1,
 - And WQO = 0.5,
 - The lowest a concentration can be recorded is <1 (half LOR).
 - Therefore, is the concentration (<1) lower than the WQO (0.5)?
 - It is impossible to know.
 
The below code chunk will flag any site that has:
 - an LOR <= WQO and,
 - more than half of the concentration values also <= WQO (i.e. cannot be interpreted). 
 
For any site that is flagged, the offending indicator will be removed.

```{r}
#| label: LOR WQO check
#| warning: true

#check if the LOR is equal to or greater than the WQO, whilst also checking if any values are also in this range
wqo_lor_check_p1 <- estuarine_wq_all |> 
  filter(FY == current_fyear, Indicator != LOR) |> #this gets current year and ignore FRP
  mutate(Comparison = case_when(LOR >= WQO & Values > LOR ~ "Val_Warn",
                                LOR >= WQO & Values <= LOR ~ "Val_Fail",
                                T ~ "Val_Pass")) |>
  group_by(Code, Indicator, LOR, WQO, Comparison) |> 
  summarise(Count = n(), .groups = "drop") |>
  pivot_wider(names_from = Comparison, values_from = Count)

#clean up the main dataset by removing ammonia and NOX as they are no longer needed after this check has been completed.
estuarine_wq_all <- estuarine_wq_all |> filter(!Indicator %in% c("Ammonia", "NOX"))

#check if warning and fail values exists
if (any("Val_Fail" %in% colnames(wqo_lor_check_p1))){

  #conduct checks for each indicator to determine if enough samples are above LOR/WQO to be allowed to use the site
  wqo_lor_check_p2 <- wqo_lor_check_p1 |> filter(Indicator != "DIN") |> 
    mutate(across(everything(), ~ replace_na(.x, 0)),
           Indicator = case_when(Indicator %in% c("Ammonia", "NOX") ~ "DIN", 
                                 T ~ Indicator)) |> 
    group_by(Code, Indicator) |> 
    summarise(across(contains("Val"), ~ sum(.x)),
              Keep_Site = case_when(Val_Fail < (if(exists("wqo_lor_check_p1$Val_Warn")){Val_Warn + Val_Pass} else {Val_Pass}) ~ "Keep",
                                    T ~ "Remove"), .groups = "drop") |> 
    ungroup()
  
  #save the lor WQO check table
  write_csv(wqo_lor_check_p2, glue("{save_path}/spreadsheets/wqo_lor_check.csv"))
  
  #check if any sites need to be removed
  if (!any(str_detect(wqo_lor_check_p2$Keep_Site, "Rem"))){
    
    #if no sites need to be removed save the data
    write_csv(estuarine_wq_all, glue("{save_path}/spreadsheets/estuarine_wq_all.csv"))
  
    print("Test passed. No sites need to be removed.")
    
  } else { #if warning and fail values outnumber values that pass then remove those rows
  
    #to remove problem sites create a table containing the code-indicator pair of any failed sites
    removal_table <- wqo_lor_check_p2 |> 
      filter(Keep_Site == "Remove") |> 
      select(Code, Indicator)
  
    #use the full dataset to add all information about the code-indicator pair of any failed sites
    removal_table <- semi_join(filter(estuarine_wq_all, FY == current_fyear), removal_table, by = join_by(Code, Indicator))
  
    #subtract from the main table, any rows that appear in the removal table
    estuarine_wq_removed <- suppressMessages(anti_join(estuarine_wq_all, removal_table))
      
    #save the before and after site removal tables
    write_csv(estuarine_wq_all, glue("{save_path}/spreadsheets/estuarine_wq_all_pre_site_removal.csv"))
    write_csv(estuarine_wq_removed, glue("{save_path}/spreadsheets/estuarine_wq_all_post_site_removal.csv"))
    
    #save the table that lists what was actually removed
    write_csv(removal_table, glue("{save_path}/spreadsheets/estuarine_wq_all_sites_removed.csv"))
    
    warning("At >= one site, the LOR is >= WQO AND >= 50% of the values are <= LOR. This means at 
    least one site has failed. The offending sites and indicators have been removed, however the 
    code will provide a warning here so the user is aware and can inspect the cause of the issue.")

  }
  
} else {#if there are or fail values
  
  #save the dataset
  write_csv(estuarine_wq_all, glue("{save_path}/spreadsheets/estuarine_wq_all.csv"))
  
  print("Test passed. No sites need to be removed.")

}    

```

The code chunk above has automatically removed the offending indicators from the offending sites. Now, in the code chunk below, we will overwrite the main freshwater dataset with the dataset we just created that has some values removed.

```{r}
#| label: overwrite old data

#if data needed to be removed, store the main dataset as "old" and assign the updated dataset back to the main dataset
if (exists("estuarine_wq_removed")){
  
  estuarine_wq_pre_removal <- estuarine_wq_all
  
  estuarine_wq_all <- estuarine_wq_removed}

```

# EDA Checks

After some mostly automated QA/QC we will now conduct some exploratory data analysis to see if we can spot anything wrong with this years data.

```{r}
#| label: select this years data

estuarine_wq_cy <- estuarine_wq_all |> filter(FY == current_fyear)

```

## Histograms

Next we will create histograms for this years data, e.g.:

```{r}
#| label: EDA histograms

for (i in unique(estuarine_wq_cy$Indicator)){#for each indicator
  
  #get only that indicator
  target_data <- estuarine_wq_cy |> filter(Indicator == i)
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Values, color = Code, fill = Code)) +
    geom_histogram(alpha = 0.6, bins = 100) +
    viridis::scale_fill_viridis(discrete = T) +
    viridis::scale_colour_viridis(discrete = T) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
      xlab(glue("{i}")) +
      ylab("Count") +
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{save_path}/graphs/{i}_histogram.png"))
  
}

#show last plot
temp_plot

```

## Boxplots

and as a box plots, e.g.:

```{r}
#| label: EDA Boxplots

#create custom log function for tick breaks
base_breaks <- function(n = 10){
    function(x) {
        axisTicks(log10(range(x, na.rm = TRUE)), log = TRUE, n = n)
    }
}

for (i in unique(estuarine_wq_cy$Indicator)){#for each indicator
  
  #get only that indicator
  target_data <- estuarine_wq_cy |> filter(Indicator == i)
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Indicator, y = Values, fill = Code)) +
    geom_boxplot(alpha = 0.6) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
    xlab("") +
    scale_y_continuous(trans = scales::log_trans(), breaks = base_breaks(),
                   labels = prettyNum) +
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{save_path}/graphs/{i}_boxplot.png"))
  
}

#show last plot
temp_plot

```

## Lineplots

and some line plots, e.g.:

```{r}
#| label: EDA line plots

for (i in unique(estuarine_wq_cy$Indicator)){#for each indicator

  #get only that indicator and convert from ymd_hms to ymd only
  target_data <- estuarine_wq_cy |> filter(Indicator == i) |> mutate(Date = ymd(substr(Date, 1, 10)))
  
  if (str_detect(i, "DO")){#if the indicator is either High DO or Low DO, rename to just DO
    target_data <- target_data |> mutate(Indicator == "DO")
    i <- "DO"
  }
  
  #plot data
  temp_plot <- ggplot(target_data, aes(x = Date, y = Values, color = Code)) +
    geom_line() +
    geom_point(size = 0.4) +
    theme_bw() +
    theme(legend.position = "none",
          panel.spacing = unit(0.1, "lines"),
          strip.text.x = element_text(size = 8)) +
    xlab(glue("{i}")) +
    scale_x_date(breaks = scales::pretty_breaks(n = 2))+
    facet_wrap(~Code, scales = "free")
  
  #save to file
  ggsave(glue("{save_path}/graphs/{i}_lineplot.png"))
  
}

#show last plot
temp_plot

```

# Action Outstanding EDA

Earlier in the script we conducted preliminary QA/QC and were also able to automated some of the common actions required. However after conducting preliminary EDA there is no easy way to automate the actions required by the findings of this. Thus this section is the custom response to the EDA findings that were determined for the **2023** (financial year) data.

**It is important to note that this section should manually be rewritten each year to specifically address the EDA finding of the year.**

To make sure we don't miss anything we will go indicator by indicator:

 - DIN: Nothing major to note.
    + One "very high" value recorded in BOH3.9 but still well within expected range of values.
 - DO: Nothing major.
    + LOU and TC4A showed one very low DO value (~30), however these occured roughly in sync and are still within expected ranges.
 - TP: Nothing major.
    + Interesting to note that RC07 does not vary at all for TP, despite surrounding sites varying (and it moving in sync with these sites for other variables).
 - Turbidity: Nothing major.
    + Values peaked around 125 for stuart creek, however  this is still within expected turbidity ranges.


```{r}
#| label: action EDA findings

print("This year, no EDA findings required action.")

```

# Save Data

Now that all QA/QC and EDA actions are completed we can save the processed data in the main data folder, ready for the main analysis script. This save step is quite important as we dont want to overwrite old dataset prematurely, thus we will include the year of data that was targeted for the eda checks.

Also, once again it should be noted that the dataset may have had some values removed during the LOR/WQO QA/QC step. If this is the case the dataset will contain an extra bit of information in the file name stating as such.

```{r}
#| label: save data to main data folder

if (exists("estuarine_wq_removed")) {# a removed and non-removed set exists, save both
  
  #save the data with the removed sites
  write_csv(estuarine_wq_all, 
            glue("{here()}/data/dt_water-quality_estuarine/processed/{current_fyear-1}-{current_fyear}_estuarine_wq_all_sites_removed.csv"))
  
  #save the original without the removed sites - this is useful for inspecting at a later date
  write_csv(estuarine_wq_pre_removal, 
            glue("{here()}/data/dt_water-quality_estuarine/processed/{current_fyear-1}-{current_fyear}_estuarine_wq_all_pre_removal.csv"))

} else {#otherwise just save the one
  
  write_csv(estuarine_wq_all, 
            glue("{here()}/data/dt_water-quality_estuarine/processed/{current_fyear-1}-{current_fyear}_estuarine_wq_all.csv"))

}

```

